# READ ME

### Technical:

1. Make sure you first install the requirements.txt:
   1. pip install -r requirements.txt
2. The code was developed using Keras with GPU-enabled Tensorflow as backend, CUDA  V9.0.176 and cudNN V7.1.4

### Introduction:

The purpose of this project was to explore the options of implementing abstractive summarization models in Python. The domain we chose was the Biomedical abstracts, mainly, papers on clinical studies. The data we used is the [PubMed 200k RCT dataset](https://github.com/Franck-Dernoncourt/pubmed-rct) generated by Franck Dernoncourt and Ji Young Lee.

4 models were generated with the top model producing the ROUGE1 score of around 15 in our evaluations. The main limitations we ran into was the hardware we used for training the models. The Models and the theory behind them are further discussed in the '**Summarization of Medical Abstracts**' presentation, included in the '**Presentations**' folder.

### Instructions:

- With the exception of '**model1**' and '**preproc**' notebooks, all of the required files for running the notebooks, training the models and making the predictions are included.
- For the 'model1' notebook (which creates the least useful summarization model), the corpus was not included as it was very large. This corpus can be created by the use of the '**preproc**' notebook, after the raw data is downloaded (instructions in the notebook).
- Notebooks for models 2 through 4 train the summarization model from scratch and demonstrate the predictions; training roughly takes about 3-4 hours per model on a GTX 1060 6GB laptop version.
- The notebooks are self-documented; all of them simply need to be executed from top to bottom to demonstrate the complete capabilities of the code and the models.
- In order to avoid training the models again, the '**Prediction test**' notebook can demonstrate and evaluate the prediction capabilities of our best model, which is pretrained and included in the directory ('**model4_pretrained**').

### References

- *Franck Dernoncourt, Ji Young Lee.Â PubMed 200k RCT: a Dataset for Sequential Sentence Classification in Medical Abstracts. International Joint Conference on Natural Language Processing (IJCNLP). 2017.*. 

- https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html

- https://medium.com/@JMangia/coreml-with-glove-word-embedding-and-recursive-neural-network-part-2-ab238ca90970

- https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html

- https://towardsdatascience.com/neural-machine-translation-using-seq2seq-with-keras-c23540453c74

- http://www.abigailsee.com/2017/04/16/taming-rnns-for-better-summarization.html

- https://arxiv.org/pdf/1602.06023.pdf

- https://web.stanford.edu/class/cs224n/reports/2749095.pdf

- https://cs224d.stanford.edu/reports/lucilley.pdf

- https://machinelearningmastery.com/encoder-decoder-models-text-summarization-keras/

  



