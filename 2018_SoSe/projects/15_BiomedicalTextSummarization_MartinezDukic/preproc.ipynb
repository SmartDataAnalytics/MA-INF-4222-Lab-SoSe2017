{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original raw data repo: https://github.com/Franck-Dernoncourt/pubmed-rct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First download the raw data from [here](https://github.com/Franck-Dernoncourt/pubmed-rct/raw/master/PubMed_200k_RCT/train.7z) and unzip into the same folder where this notebook is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input and preproc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "abstracts = dict()\n",
    "roles = ['OBJECTIVE', 'BACKGROUND', 'METHODS', 'CONCLUSIONS', 'RESULTS', 'METHODS/DESIGN', 'DISCUSSION', \n",
    "         'TRIAL REGISTRATION', 'SUMMARY']\n",
    "\n",
    "sentences = list()\n",
    "\n",
    "with open(r'D:\\nlpHW\\project\\train.txt', 'r') as fh:\n",
    "    \n",
    "    tmpSents = list()\n",
    "    tmpId = ''\n",
    "    \n",
    "    for line in fh.readlines():   \n",
    "        \n",
    "        if line.startswith('###'):\n",
    "            \n",
    "            if tmpId:\n",
    "                \n",
    "                abstracts[tmpId] = tmpSents\n",
    "                tmpId = line.strip().replace('###', '')\n",
    "                tmpSents = list()\n",
    "                \n",
    "            else:\n",
    "                \n",
    "                tmpId = line.strip().replace('###', '')\n",
    "                abstracts[tmpId] = None\n",
    "        else:\n",
    "            \n",
    "            sentences.append([token.strip() for token in line.split() if token not in roles])\n",
    "            tmpSents.append([token.strip() for token in line.split() if token not in roles])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pmids = list(abstracts.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pmids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(abstracts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "abstracts['24491034']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "visualize the abstract length distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "absLengths = [len(s) for a,s in abstracts.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(absLengths)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.mean(absLengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(r'abstractsCorpus.pkl', 'wb') as fh:\n",
    "    pickle.dump(abstractsCorpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the longest training data instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "max([len(list(chain.from_iterable(abstractsCorpus[each]))) for each in abstractsCorpus]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a word2vec with only half the corpus; cant fit in memory otherwise, too many unique words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = list()\n",
    "for each in list(abstractsCorpus.values())[:10000]:\n",
    "    for s in each:\n",
    "        sentences.append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook\n",
    "from itertools import chain\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "sents = list()\n",
    "bar = tqdm_notebook(list(enumerate(sentences)))\n",
    "for i, _ in bar:\n",
    "    sents.append([wordnet_lemmatizer.lemmatize(word).strip(r'.,:-?_!])').lower() for word in sentences[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Longest abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max([len(list(chain.from_iterable(abstractsCorpus[each]))) for each in list(abstractsCorpus.keys())[:10000]]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The w2v on corpus (abstracts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec(sents, min_count=1)\n",
    "model.save('PubMed_200k_RCT_model_10000')\n",
    "print('PubMed 200k RCT corpus model saved.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec.load('PubMed_200k_RCT_model_10000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.most_similar('result'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetching the titles for the abstracts\n",
    "## (based on the pmids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from Bio import Entrez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Entrez.email = 'd.dejan.djukic@gmail.com'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def chunks(l, n):\n",
    "    \"\"\"Yield successive n-sized chunks from l.\"\"\"\n",
    "    for i in range(0, len(l), n):\n",
    "        yield l[i:i + n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook\n",
    "titles = dict()\n",
    "notFound = list()\n",
    "\n",
    "bar = tqdm_notebook(list(chunks(pmids, 10000)))\n",
    "\n",
    "for each in bar:\n",
    "    \n",
    "    handle = Entrez.efetch(db=\"pubmed\", id=each, retmode = 'xml')\n",
    "    try:\n",
    "        rec = Entrez.read(handle)\n",
    "\n",
    "        bar2 = tqdm_notebook(list(zip(each, [record for record in rec['PubmedArticle']])))\n",
    "        for pmid, record in bar2:\n",
    "\n",
    "            try:\n",
    "                \n",
    "                title = record[\"MedlineCitation\"][\"Article\"][\"ArticleTitle\"]\n",
    "                if title:\n",
    "                    titles[pmid] = title\n",
    "                else:\n",
    "                    notFound.append(pmid)\n",
    "                \n",
    "            except:\n",
    "                notFound.append(pmid)\n",
    "    except:\n",
    "        notFound.append(pmid)\n",
    "\n",
    "    \n",
    "    print(f'current size of the titles dict: {len(titles)}')\n",
    "    print(f'titles not found for: {notFound}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('titlesAbstracts.pkl', 'wb') as fh:\n",
    "    pickle.dump(titles)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
