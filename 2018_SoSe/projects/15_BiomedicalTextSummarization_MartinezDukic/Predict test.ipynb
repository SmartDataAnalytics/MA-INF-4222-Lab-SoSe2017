{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Based on the best model we have trained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from collections import Counter\n",
    "import pickle\n",
    "from itertools import chain\n",
    "\n",
    "import gc\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.test.utils import get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers import Embedding, Dense, Input, RepeatVector, TimeDistributed, concatenate, add, Dropout\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%config InteractiveShell.ast_node_interactivity = 'all'\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.Session(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#defaults for params\n",
    "\n",
    "MAX_INPUT_SEQ_LENGTH = 500\n",
    "MAX_TARGET_SEQ_LENGTH = 50\n",
    "MAX_INPUT_VOCAB_SIZE = 5000\n",
    "MAX_TARGET_VOCAB_SIZE = 2000\n",
    "\n",
    "def fit_text(X, Y, input_seq_max_length=None, target_seq_max_length=None):\n",
    "    \"\"\"Creates the dictionaries for the word to id lookup and vice versa,\n",
    "    calculates the maximum input and output sequence length and the \n",
    "    number of tokens in the dictionary\"\"\"\n",
    "    \n",
    "    if input_seq_max_length is None:\n",
    "        input_seq_max_length = MAX_INPUT_SEQ_LENGTH\n",
    "    if target_seq_max_length is None:\n",
    "        target_seq_max_length = MAX_TARGET_SEQ_LENGTH\n",
    "    input_counter = Counter()\n",
    "    target_counter = Counter()\n",
    "    max_input_seq_length = 0\n",
    "    max_target_seq_length = 0\n",
    "\n",
    "    for line in X:\n",
    "        text = [word.lower() for word in line.split(' ')]\n",
    "        seq_length = len(text)\n",
    "        if seq_length > input_seq_max_length:\n",
    "            text = text[0:input_seq_max_length]\n",
    "            seq_length = len(text)\n",
    "        for word in text:\n",
    "            input_counter[word] += 1\n",
    "        max_input_seq_length = max(max_input_seq_length, seq_length)\n",
    "\n",
    "    for line in Y:\n",
    "        line2 = 'START ' + line.lower() + ' END'\n",
    "        text = [word for word in line2.split(' ')]\n",
    "        seq_length = len(text)\n",
    "        if seq_length > target_seq_max_length:\n",
    "            text = text[0:target_seq_max_length]\n",
    "            seq_length = len(text)\n",
    "        for word in text:\n",
    "            target_counter[word] += 1\n",
    "            max_target_seq_length = max(max_target_seq_length, seq_length)\n",
    "\n",
    "    input_word2idx = dict()\n",
    "    for idx, word in enumerate(input_counter.most_common(MAX_INPUT_VOCAB_SIZE)):\n",
    "        input_word2idx[word[0]] = idx + 2\n",
    "    input_word2idx['PAD'] = 0\n",
    "    input_word2idx['UNK'] = 1\n",
    "    input_idx2word = dict([(idx, word) for word, idx in input_word2idx.items()])\n",
    "\n",
    "    target_word2idx = dict()\n",
    "    for idx, word in enumerate(target_counter.most_common(MAX_TARGET_VOCAB_SIZE)):\n",
    "        target_word2idx[word[0]] = idx + 1\n",
    "    target_word2idx['UNK'] = 0\n",
    "\n",
    "    target_idx2word = dict([(idx, word) for word, idx in target_word2idx.items()])\n",
    "    \n",
    "    num_input_tokens = len(input_word2idx)\n",
    "    num_target_tokens = len(target_word2idx)\n",
    "\n",
    "    config = dict()\n",
    "    config['input_word2idx'] = input_word2idx\n",
    "    config['input_idx2word'] = input_idx2word\n",
    "    config['target_word2idx'] = target_word2idx\n",
    "    config['target_idx2word'] = target_idx2word\n",
    "    config['num_input_tokens'] = num_input_tokens\n",
    "    config['num_target_tokens'] = num_target_tokens\n",
    "    config['max_input_seq_length'] = max_input_seq_length\n",
    "    config['max_target_seq_length'] = max_target_seq_length\n",
    "\n",
    "    return config\n",
    "\n",
    "def summarize(input_text):\n",
    "    \"\"\"Creates the summary from the input sequence;\n",
    "    samples from the decoder until either the end token is reached\n",
    "    or the maximum output sequence length is reached\"\"\"\n",
    "    \n",
    "    input_seq = []\n",
    "    input_wids = []\n",
    "    for word in input_text.lower().split(' '):\n",
    "        idx = 1  # default [UNK]\n",
    "        if word in  input_word2idx:\n",
    "            idx =  input_word2idx[word]\n",
    "        input_wids.append(idx)\n",
    "    input_seq.append(input_wids)\n",
    "    input_seq = pad_sequences(input_seq,  max_input_seq_length)\n",
    "    start_token =  target_word2idx['START']\n",
    "    wid_list = [start_token]\n",
    "    sum_input_seq = pad_sequences([wid_list], min( num_target_tokens, MAX_DECODER_SEQ_LENGTH))\n",
    "    terminated = False\n",
    "\n",
    "    target_text = ''\n",
    "\n",
    "    while not terminated:\n",
    "        output_tokens =  model.predict([input_seq, sum_input_seq])\n",
    "        sample_token_idx = np.argmax(output_tokens[0, :])\n",
    "        sample_word =  target_idx2word[sample_token_idx]\n",
    "        wid_list = wid_list + [sample_token_idx]\n",
    "\n",
    "        if sample_word != 'START' and sample_word != 'END':\n",
    "            target_text += ' ' + sample_word\n",
    "        if sample_word == 'END' or len(wid_list) >=  max_target_seq_length:\n",
    "            terminated = True\n",
    "        else:\n",
    "            sum_input_seq = pad_sequences([wid_list], min( num_target_tokens, MAX_DECODER_SEQ_LENGTH))\n",
    "    return target_text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model(r'model4_pretrained') #if exists; point to your path if different\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('titlesAbstracts_AT.pkl', 'rb') as fh:\n",
    "    titles = pickle.load(fh)\n",
    "\n",
    "with open('abstractsCorpus_ATsigns.pkl', 'rb') as fh:\n",
    "    text = pickle.load(fh)\n",
    "\n",
    "textConcat = list()\n",
    "\n",
    "from itertools import chain\n",
    "for each in text.values():\n",
    "    tmp = ' '.join(list(chain.from_iterable(each)))\n",
    "    textConcat.append(tmp)\n",
    "\n",
    "X = textConcat[:10000]\n",
    "Y = list(titles.values())[:10000]\n",
    "\n",
    "Xfull = textConcat\n",
    "Yfull = list(titles.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = fit_text(X, Y)\n",
    "\n",
    "HIDDEN_UNITS = 100\n",
    "\n",
    "MAX_DECODER_SEQ_LENGTH = 4\n",
    "\n",
    "input_word2idx = conf['input_word2idx']\n",
    "input_idx2word = conf['input_idx2word']\n",
    "target_word2idx = conf['target_word2idx'] \n",
    "target_idx2word = conf['target_idx2word']\n",
    "num_input_tokens = conf['num_input_tokens']\n",
    "num_target_tokens = conf['num_target_tokens']\n",
    "max_input_seq_length = conf['max_input_seq_length']\n",
    "max_target_seq_length = conf['max_target_seq_length']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get random article to test the prediction on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "c = randint(10000, len(Xfull))\n",
    "\n",
    "textPredict = Xfull[c]\n",
    "labelPredict = Yfull[c]\n",
    "\n",
    "summarize(textPredict)\n",
    "labelPredict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sumeval.metrics.rouge import RougeCalculator\n",
    "rouge = RougeCalculator(stopwords=True, lang=\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rouge1 = dict()\n",
    "\n",
    "for _ in range(50):\n",
    "    from random import randint\n",
    "    c = randint(10000, len(Xfull))\n",
    "\n",
    "    textPredict = Xfull[c]\n",
    "    labelPredict = Yfull[c]\n",
    "\n",
    "    generated = summarize(textPredict)\n",
    "    reference = labelPredict\n",
    "\n",
    "    score = rouge.rouge_n(\n",
    "                summary=generated,\n",
    "                references=reference,\n",
    "                n=1)\n",
    "    \n",
    "    rouge1[score] = (generated, reference)\n",
    "\n",
    "for s in sorted(rouge1.keys(), reverse=True):\n",
    "    \n",
    "    gen, orig = rouge1[s]\n",
    "    print(s)\n",
    "    print(f'Generated headline:{gen}')\n",
    "    print(f'Original headline:{orig}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rouge1 avg:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.mean(list(rouge1.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (For reference) Rouge1 example on ideal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rouge.rouge_n(summary='I would like an apple.', references='I would like an apple.', n=1)\n",
    "rouge.rouge_n(summary='I would like to eat an apple.', references='I feel like having an apple.', n=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
