{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from collections import Counter\n",
    "import pickle\n",
    "from itertools import chain\n",
    "\n",
    "import gc\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.test.utils import get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers import Embedding, Dense, Input, RepeatVector, TimeDistributed, concatenate, add, Dropout\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%config InteractiveShell.ast_node_interactivity = 'all'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Necessary for tf to grab as much memory as is available, not as much as it wants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.Session(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_embeddings(w2v_model_path, embeddings_path):\n",
    "    \"\"\"Imports the the previously trained word2vec\n",
    "    model and procedes to extract and save to disk the embeddings \n",
    "    weights/vectors as a numpy matrix. \"\"\"\n",
    "    \n",
    "    model = Word2Vec.load(w2v_model_path)   \n",
    "    weights = model.wv.syn0\n",
    "    np.save(open(embeddings_path, 'wb'), weights)\n",
    "\n",
    "\n",
    "def update_dicts(w2v_model_path):\n",
    "    \"\"\"Uses a word2vec model's vocabulary to\n",
    "    construct word-index and index-word mappings, it also adds\n",
    "    special tokens as the lowerest indeces\"\"\"\n",
    "    \n",
    "    model = Word2Vec.load(w2v_model_path)\n",
    "   \n",
    "    word2idx = dict([(k, v.index) for k, v in model.wv.vocab.items()])\n",
    "    word2idx['<unk>'] = len(word2idx)\n",
    "        \n",
    "    idx2word = dict([(v, k) for k, v in word2idx.items()])\n",
    "    \n",
    "    return word2idx, idx2word\n",
    "\n",
    "\n",
    "def create_embeddings_layer(embeddings_path):\n",
    "    \"\"\"Creates and embeddings layer fromt the\n",
    "    weights matrix of a word2vec model\"\"\"\n",
    "    \n",
    "    weights = np.load(open(embeddings_path, 'rb'))\n",
    "   \n",
    "    layer = Embedding(input_dim=weights.shape[0],\n",
    "                      output_dim=weights.shape[1],\n",
    "                      weights=[weights], trainable=False)\n",
    "    print(\"(vocab_size, output_dim) : \", weights.shape)\n",
    "    return layer\n",
    "\n",
    "#Retrieving Data Necessary to Build the Embedding Layer\n",
    "abs_model_path = 'PubMed_200k_RCT_model_10000'\n",
    "abs_embed_path = 'Abstracts_embeddings.plz' #save path\n",
    "\n",
    "#abstracts\n",
    "extract_embeddings(abs_model_path, abs_embed_path)\n",
    "\n",
    "#abstract word-ID map dictionaries update\n",
    "ABS_word2idx, ABS_idx2word = update_dicts(abs_model_path)\n",
    "\n",
    "#dictionary length\n",
    "print(\"Dictionary Length: \",len(ABS_idx2word.values()))\n",
    "\n",
    "def loadW2V(w2vPath):\n",
    "    \n",
    "    from gensim.models import Word2Vec\n",
    "    \n",
    "    model = Word2Vec.load(w2vPath)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def transform_input_text(texts, abs_model_path):\n",
    "    \"\"\"Transforms the input text so that the words are arrays padded\n",
    "    to the maximum allowed input length\"\"\"\n",
    "    \n",
    "    from keras.preprocessing.sequence import pad_sequences\n",
    "    \n",
    "    unknown_emb = np.random.rand(1, EMBEDDING_SIZE)\n",
    "    w2v = loadW2V(abs_model_path)\n",
    "    \n",
    "    temp = []\n",
    "    for line in texts:\n",
    "        x = np.zeros(shape=(max_input_seq_length, EMBEDDING_SIZE))\n",
    "        for idx, word in enumerate(line.lower().split(' ')):\n",
    "            if idx >= max_input_seq_length:\n",
    "                break\n",
    "            emb = unknown_emb\n",
    "            if word in w2v:\n",
    "                emb = w2v[word]\n",
    "            x[idx, :] = emb\n",
    "        temp.append(x)\n",
    "    temp = pad_sequences(temp, maxlen=max_input_seq_length)\n",
    "\n",
    "    print(temp.shape)\n",
    "    return temp\n",
    "\n",
    "def transform_target_encoding(texts):\n",
    "    \"\"\"Splits the target text and adds the start\n",
    "    and end tokens\"\"\"\n",
    "    \n",
    "    temp = []\n",
    "    for line in texts:\n",
    "        x = []\n",
    "        line2 = 'START ' + line.lower() + ' END'\n",
    "        for word in line2.split(' '):\n",
    "            x.append(word)\n",
    "            if len(x)+1 >= max_target_seq_length:\n",
    "                break\n",
    "        temp.append(x)\n",
    "\n",
    "    temp = np.array(temp)\n",
    "    return temp\n",
    "\n",
    "#defaults for params\n",
    "\n",
    "MAX_INPUT_SEQ_LENGTH = 500\n",
    "MAX_TARGET_SEQ_LENGTH = 50\n",
    "MAX_INPUT_VOCAB_SIZE = 5000\n",
    "MAX_TARGET_VOCAB_SIZE = 2000\n",
    "\n",
    "def fit_text(X, Y, input_seq_max_length=None, target_seq_max_length=None):\n",
    "    \"\"\"Creates the dictionaries for the word to id lookup and vice versa,\n",
    "    calculates the maximum input and output sequence length and the \n",
    "    number of tokens in the dictionary\"\"\"\n",
    "    \n",
    "    if input_seq_max_length is None:\n",
    "        input_seq_max_length = MAX_INPUT_SEQ_LENGTH\n",
    "    if target_seq_max_length is None:\n",
    "        target_seq_max_length = MAX_TARGET_SEQ_LENGTH\n",
    "    input_counter = Counter()\n",
    "    target_counter = Counter()\n",
    "    max_input_seq_length = 0\n",
    "    max_target_seq_length = 0\n",
    "\n",
    "    for line in X:\n",
    "        text = [word.lower() for word in line.split(' ')]\n",
    "        seq_length = len(text)\n",
    "        if seq_length > input_seq_max_length:\n",
    "            text = text[0:input_seq_max_length]\n",
    "            seq_length = len(text)\n",
    "        for word in text:\n",
    "            input_counter[word] += 1\n",
    "        max_input_seq_length = max(max_input_seq_length, seq_length)\n",
    "\n",
    "    for line in Y:\n",
    "        line2 = 'START ' + line.lower() + ' END'\n",
    "        text = [word for word in line2.split(' ')]\n",
    "        seq_length = len(text)\n",
    "        if seq_length > target_seq_max_length:\n",
    "            text = text[0:target_seq_max_length]\n",
    "            seq_length = len(text)\n",
    "        for word in text:\n",
    "            target_counter[word] += 1\n",
    "            max_target_seq_length = max(max_target_seq_length, seq_length)\n",
    "\n",
    "    input_word2idx = dict()\n",
    "    for idx, word in enumerate(input_counter.most_common(MAX_INPUT_VOCAB_SIZE)):\n",
    "        input_word2idx[word[0]] = idx + 2\n",
    "    input_word2idx['PAD'] = 0\n",
    "    input_word2idx['UNK'] = 1\n",
    "    input_idx2word = dict([(idx, word) for word, idx in input_word2idx.items()])\n",
    "\n",
    "    target_word2idx = dict()\n",
    "    for idx, word in enumerate(target_counter.most_common(MAX_TARGET_VOCAB_SIZE)):\n",
    "        target_word2idx[word[0]] = idx + 1\n",
    "    target_word2idx['UNK'] = 0\n",
    "\n",
    "    target_idx2word = dict([(idx, word) for word, idx in target_word2idx.items()])\n",
    "    \n",
    "    num_input_tokens = len(input_word2idx)\n",
    "    num_target_tokens = len(target_word2idx)\n",
    "\n",
    "    config = dict()\n",
    "    config['input_word2idx'] = input_word2idx\n",
    "    config['input_idx2word'] = input_idx2word\n",
    "    config['target_word2idx'] = target_word2idx\n",
    "    config['target_idx2word'] = target_idx2word\n",
    "    config['num_input_tokens'] = num_input_tokens\n",
    "    config['num_target_tokens'] = num_target_tokens\n",
    "    config['max_input_seq_length'] = max_input_seq_length\n",
    "    config['max_target_seq_length'] = max_target_seq_length\n",
    "\n",
    "    return config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that due to the size constraints, the two pickle files needed for the cell bellow to run were not uploaded; they can be generated by using the 'preproc' notebook after downloading the raw data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('titlesAbstracts.pkl', 'rb') as fh:\n",
    "    titles = pickle.load(fh)\n",
    "\n",
    "with open('abstractsCorpus.pkl', 'rb') as fh:\n",
    "    text = pickle.load(fh)\n",
    "\n",
    "textConcat = list()\n",
    "\n",
    "from itertools import chain\n",
    "for each in text.values():\n",
    "    tmp = ' '.join(list(chain.from_iterable(each)))\n",
    "    textConcat.append(tmp)\n",
    "\n",
    "X = textConcat[:10000]\n",
    "Y = list(titles.values())[:10000]\n",
    "\n",
    "Xfull = textConcat\n",
    "Yfull = list(titles.values())\n",
    "\n",
    "Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# set up the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Params:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = np.load(open(abs_embed_path, 'rb'))\n",
    "vocab_size = weights.shape[0]\n",
    "EMBEDDING_SIZE = weights.shape[1]\n",
    "max_input_seq_length = 500 #the longest abstract (calculated in prepoc); the + 100 part is to account for added tokens\n",
    "num_input_tokens = vocab_size\n",
    "num_target_tokens = vocab_size\n",
    "max_target_seq_length = 50\n",
    "HIDDEN_UNITS = 100\n",
    "default_batch_size = 64\n",
    "default_epochs = 2\n",
    "verbose = 1\n",
    "\n",
    "conf = fit_text(X, Y)\n",
    "\n",
    "input_word2idx = conf['input_word2idx']\n",
    "input_idx2word = conf['input_idx2word']\n",
    "target_word2idx = conf['target_word2idx'] \n",
    "target_idx2word = conf['target_idx2word']\n",
    "num_input_tokens = conf['num_input_tokens']\n",
    "num_target_tokens = conf['num_target_tokens']\n",
    "max_input_seq_length = conf['max_input_seq_length']\n",
    "max_target_seq_length = conf['max_target_seq_length']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_inputs = Input(shape=(None, EMBEDDING_SIZE), name='encoder_inputs')\n",
    "encoder_lstm = LSTM(units=HIDDEN_UNITS, return_state=True, name='encoder_lstm')\n",
    "encoder_outputs, encoder_state_h, encoder_state_c = encoder_lstm(encoder_inputs)\n",
    "encoder_states = [encoder_state_h, encoder_state_c]\n",
    "\n",
    "decoder_inputs = Input(shape=(None, num_target_tokens), name='decoder_inputs')\n",
    "decoder_lstm = LSTM(units=HIDDEN_UNITS, return_state=True, return_sequences=True, name='decoder_lstm')\n",
    "decoder_outputs, decoder_state_h, decoder_state_c = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "\n",
    "decoder_dense = Dense(units=num_target_tokens, activation='softmax', name='decoder_dense')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_state_inputs = [Input(shape=(HIDDEN_UNITS,)), Input(shape=(HIDDEN_UNITS,))]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=decoder_state_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model([decoder_inputs] + decoder_state_inputs, [decoder_outputs] + decoder_states)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_batch(x_samples, y_samples, batch_size):\n",
    "    \"\"\"Generates the batches from the training data given the \n",
    "    batch size necessary; pads sequences if required\"\"\"\n",
    "    \n",
    "    num_batches = len(x_samples) // batch_size\n",
    "    while True:\n",
    "        for batchIdx in range(0, num_batches):\n",
    "            start = batchIdx * batch_size\n",
    "            end = (batchIdx + 1) * batch_size\n",
    "            encoder_input_data_batch = pad_sequences(x_samples[start:end],  max_input_seq_length)\n",
    "            decoder_target_data_batch = np.zeros(shape=(batch_size,  max_target_seq_length,  num_target_tokens))\n",
    "            decoder_input_data_batch = np.zeros(shape=(batch_size,  max_target_seq_length,  num_target_tokens))\n",
    "            for lineIdx, target_words in enumerate(y_samples[start:end]):\n",
    "                for idx, w in enumerate(target_words):\n",
    "                    w2idx = 0  # default [UNK]\n",
    "                    if w in target_word2idx:\n",
    "                        w2idx = target_word2idx[w]\n",
    "                    if w2idx != 0:\n",
    "                        decoder_input_data_batch[lineIdx, idx, w2idx] = 1\n",
    "                        if idx > 0:\n",
    "                            decoder_target_data_batch[lineIdx, idx - 1, w2idx] = 1\n",
    "            yield [encoder_input_data_batch, decoder_input_data_batch], decoder_target_data_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fit(Xtrain, Ytrain, Xtest, Ytest, epochs=None, batch_size=None, model_dir_path=None):\n",
    "    \"\"\"Trains a model and returns a history so that the training\n",
    "    statistics could be monitored as well\"\"\" \n",
    "    \n",
    "    if epochs is None:\n",
    "        epochs = DEFAULT_EPOCHS\n",
    "    if model_dir_path is None:\n",
    "        model_dir_path = 'models_ATsigns'\n",
    "    if batch_size is None:\n",
    "        batch_size = DEFAULT_BATCH_SIZE\n",
    "    \n",
    "    checkpoint = ModelCheckpoint('models_ATsigns')\n",
    "    \n",
    "    Ytrain =  transform_target_encoding(Ytrain)\n",
    "    Ytest =  transform_target_encoding(Ytest)\n",
    "\n",
    "    Xtrain =  transform_input_text(Xtrain, abs_model_path)\n",
    "    Xtest =  transform_input_text(Xtest, abs_model_path)\n",
    "\n",
    "    train_gen =  generate_batch(Xtrain, Ytrain, batch_size)\n",
    "    test_gen =  generate_batch(Xtest, Ytest, batch_size)\n",
    "\n",
    "    train_num_batches = len(Xtrain) // batch_size\n",
    "    test_num_batches = len(Xtest) // batch_size\n",
    "\n",
    "    history =  model.fit_generator(generator=train_gen, steps_per_epoch=train_num_batches,\n",
    "                                       epochs=epochs,\n",
    "                                       verbose=1, validation_data=test_gen, validation_steps=test_num_batches,\n",
    "                                       callbacks=[checkpoint])\n",
    "    model.save('models_ATsigns')\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def summarize(input_text):\n",
    "    \"\"\"Creates the summary from the input sequence;\n",
    "    samples from the decoder until either the end token is reached\n",
    "    or the maximum output sequence length is reached\"\"\"\n",
    "    \n",
    "    unknown_emb = np.random.rand(1, EMBEDDING_SIZE)\n",
    "    w2v = loadW2V(abs_model_path)\n",
    "    \n",
    "    input_seq = np.zeros(shape=(1,  max_input_seq_length, EMBEDDING_SIZE))\n",
    "    for idx, word in enumerate(input_text.lower().split(' ')):\n",
    "        if idx >=  max_input_seq_length:\n",
    "            break\n",
    "        emb =  unknown_emb  # default [UNK]\n",
    "        if word in w2v:\n",
    "            emb =  w2v[word]\n",
    "        input_seq[0, idx, :] = emb\n",
    "    states_value =  encoder_model.predict(input_seq)\n",
    "    target_seq = np.zeros((1, 1,  num_target_tokens))\n",
    "    target_seq[0, 0,  target_word2idx['START']] = 1\n",
    "    target_text = ''\n",
    "    target_text_len = 0\n",
    "    terminated = False\n",
    "    while not terminated:\n",
    "        output_tokens, h, c =  decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "        sample_token_idx = np.argmax(output_tokens[0, -1, :])\n",
    "        sample_word =  target_idx2word[sample_token_idx]\n",
    "        target_text_len += 1\n",
    "\n",
    "        if sample_word != 'START' and sample_word != 'END':\n",
    "            target_text += ' ' + sample_word\n",
    "\n",
    "        if sample_word == 'END' or target_text_len >=  max_target_seq_length:\n",
    "            terminated = True\n",
    "\n",
    "        target_seq = np.zeros((1, 1,  num_target_tokens))\n",
    "        target_seq[0, 0, sample_token_idx] = 1\n",
    "\n",
    "        states_value = [h, c]\n",
    "    return target_text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = fit(Xtrain, Ytrain, Xtest, Ytest, epochs=5, batch_size=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Note that the prediction here is mostly useless and just for show as the model progresses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conf = fit_text(X, Y)\n",
    "\n",
    "HIDDEN_UNITS = 100\n",
    "\n",
    "MAX_DECODER_SEQ_LENGTH = 4\n",
    "\n",
    "input_word2idx = conf['input_word2idx']\n",
    "input_idx2word = conf['input_idx2word']\n",
    "target_word2idx = conf['target_word2idx'] \n",
    "target_idx2word = conf['target_idx2word']\n",
    "num_input_tokens = conf['num_input_tokens']\n",
    "num_target_tokens = conf['num_target_tokens']\n",
    "max_input_seq_length = conf['max_input_seq_length']\n",
    "max_target_seq_length = conf['max_target_seq_length']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from random import randint\n",
    "c = randint(10000, len(Xfull))\n",
    "\n",
    "textPredict = Xfull[c]\n",
    "labelPredict = Yfull[c]\n",
    "\n",
    "labelPredict\n",
    "summarize(textPredict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
