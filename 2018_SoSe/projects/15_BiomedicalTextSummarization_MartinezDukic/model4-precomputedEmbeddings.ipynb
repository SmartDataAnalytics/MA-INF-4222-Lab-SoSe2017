{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from collections import Counter\n",
    "import pickle\n",
    "from itertools import chain\n",
    "\n",
    "import gc\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.test.utils import get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers import Embedding, Dense, Input, RepeatVector, TimeDistributed, concatenate, add, Dropout\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%config InteractiveShell.ast_node_interactivity = 'all'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Necessary for tf to grab as much memory as is available, not as much as it wants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.Session(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_embeddings(w2v_model_path, embeddings_path):\n",
    "    \"\"\"Imports the the previously trained word2vec\n",
    "    model and procedes to extract and save to disk the embeddings \n",
    "    weights/vectors as a numpy matrix. \"\"\"\n",
    "    \n",
    "    model = Word2Vec.load(w2v_model_path)   \n",
    "    weights = model.wv.syn0\n",
    "    np.save(open(embeddings_path, 'wb'), weights)\n",
    "\n",
    "\n",
    "def update_dicts(w2v_model_path):\n",
    "    \"\"\"Uses a word2vec model's vocabulary to\n",
    "    construct word-index and index-word mappings, it also adds\n",
    "    special tokens as the lowerest indeces\"\"\"\n",
    "    \n",
    "    model = Word2Vec.load(w2v_model_path)\n",
    "   \n",
    "    word2idx = dict([(k, v.index) for k, v in model.wv.vocab.items()])\n",
    "    word2idx['<unk>'] = len(word2idx)\n",
    "        \n",
    "    idx2word = dict([(v, k) for k, v in word2idx.items()])\n",
    "    \n",
    "    return word2idx, idx2word\n",
    "\n",
    "\n",
    "def create_embeddings_layer(embeddings_path):\n",
    "    \"\"\"Creates and embeddings layer fromt the\n",
    "    weights matrix of a word2vec model\"\"\"\n",
    "    \n",
    "    weights = np.load(open(embeddings_path, 'rb'))\n",
    "   \n",
    "    layer = Embedding(input_dim=weights.shape[0],\n",
    "                      output_dim=weights.shape[1],\n",
    "                      weights=[weights], trainable=False)\n",
    "    print(\"(vocab_size, output_dim) : \", weights.shape)\n",
    "    return layer\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "def transform_input_text(texts):\n",
    "    \"\"\"Transforms the input text so that the words are arrays padded\n",
    "    to the maximum allowed input length\"\"\"\n",
    "    \n",
    "    temp = []\n",
    "    for line in texts:\n",
    "        x = []\n",
    "        for word in line.lower().split(' '):\n",
    "            wid = 1\n",
    "            if word in  input_word2idx:\n",
    "                wid =  input_word2idx[word]\n",
    "            x.append(wid)\n",
    "            if len(x) >=  max_input_seq_length:\n",
    "                break\n",
    "        temp.append(x)\n",
    "    temp = pad_sequences(temp, maxlen= max_input_seq_length)\n",
    "\n",
    "    print(temp.shape)\n",
    "    return temp\n",
    "\n",
    "def split_target_text(texts):\n",
    "    \"\"\"Splits the target text and adds the start\n",
    "    and end tokens\"\"\"\n",
    "    \n",
    "    temp = []\n",
    "    for line in texts:\n",
    "        x = []\n",
    "        line2 = 'START ' + line.lower() + ' END'\n",
    "        for word in line2.split(' '):\n",
    "            x.append(word)\n",
    "            if len(x)+1 >=  max_target_seq_length:\n",
    "                x.append('END')\n",
    "                break\n",
    "        temp.append(x)\n",
    "    return temp\n",
    "\n",
    "#defaults for params\n",
    "\n",
    "MAX_INPUT_SEQ_LENGTH = 500\n",
    "MAX_TARGET_SEQ_LENGTH = 50\n",
    "MAX_INPUT_VOCAB_SIZE = 5000\n",
    "MAX_TARGET_VOCAB_SIZE = 2000\n",
    "\n",
    "def fit_text(X, Y, input_seq_max_length=None, target_seq_max_length=None):\n",
    "    \"\"\"Creates the dictionaries for the word to id lookup and vice versa,\n",
    "    calculates the maximum input and output sequence length and the \n",
    "    number of tokens in the dictionary\"\"\"\n",
    "    \n",
    "    if input_seq_max_length is None:\n",
    "        input_seq_max_length = MAX_INPUT_SEQ_LENGTH\n",
    "    if target_seq_max_length is None:\n",
    "        target_seq_max_length = MAX_TARGET_SEQ_LENGTH\n",
    "    input_counter = Counter()\n",
    "    target_counter = Counter()\n",
    "    max_input_seq_length = 0\n",
    "    max_target_seq_length = 0\n",
    "\n",
    "    for line in X:\n",
    "        text = [word.lower() for word in line.split(' ')]\n",
    "        seq_length = len(text)\n",
    "        if seq_length > input_seq_max_length:\n",
    "            text = text[0:input_seq_max_length]\n",
    "            seq_length = len(text)\n",
    "        for word in text:\n",
    "            input_counter[word] += 1\n",
    "        max_input_seq_length = max(max_input_seq_length, seq_length)\n",
    "\n",
    "    for line in Y:\n",
    "        line2 = 'START ' + line.lower() + ' END'\n",
    "        text = [word for word in line2.split(' ')]\n",
    "        seq_length = len(text)\n",
    "        if seq_length > target_seq_max_length:\n",
    "            text = text[0:target_seq_max_length]\n",
    "            seq_length = len(text)\n",
    "        for word in text:\n",
    "            target_counter[word] += 1\n",
    "            max_target_seq_length = max(max_target_seq_length, seq_length)\n",
    "\n",
    "    input_word2idx = dict()\n",
    "    for idx, word in enumerate(input_counter.most_common(MAX_INPUT_VOCAB_SIZE)):\n",
    "        input_word2idx[word[0]] = idx + 2\n",
    "    input_word2idx['PAD'] = 0\n",
    "    input_word2idx['UNK'] = 1\n",
    "    input_idx2word = dict([(idx, word) for word, idx in input_word2idx.items()])\n",
    "\n",
    "    target_word2idx = dict()\n",
    "    for idx, word in enumerate(target_counter.most_common(MAX_TARGET_VOCAB_SIZE)):\n",
    "        target_word2idx[word[0]] = idx + 1\n",
    "    target_word2idx['UNK'] = 0\n",
    "\n",
    "    target_idx2word = dict([(idx, word) for word, idx in target_word2idx.items()])\n",
    "    \n",
    "    num_input_tokens = len(input_word2idx)\n",
    "    num_target_tokens = len(target_word2idx)\n",
    "\n",
    "    config = dict()\n",
    "    config['input_word2idx'] = input_word2idx\n",
    "    config['input_idx2word'] = input_idx2word\n",
    "    config['target_word2idx'] = target_word2idx\n",
    "    config['target_idx2word'] = target_idx2word\n",
    "    config['num_input_tokens'] = num_input_tokens\n",
    "    config['num_target_tokens'] = num_target_tokens\n",
    "    config['max_input_seq_length'] = max_input_seq_length\n",
    "    config['max_target_seq_length'] = max_target_seq_length\n",
    "\n",
    "    return config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('titlesAbstracts_AT.pkl', 'rb') as fh:\n",
    "    titles = pickle.load(fh)\n",
    "\n",
    "with open('abstractsCorpus_ATsigns.pkl', 'rb') as fh:\n",
    "    text = pickle.load(fh)\n",
    "\n",
    "textConcat = list()\n",
    "\n",
    "for each in text.values():\n",
    "    tmp = ' '.join(list(chain.from_iterable(each)))\n",
    "    textConcat.append(tmp)\n",
    "\n",
    "X = textConcat[:10000]\n",
    "Y = list(titles.values())[:10000]\n",
    "\n",
    "Xfull = textConcat\n",
    "Yfull = list(titles.values())\n",
    "\n",
    "Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get the word id dict and vice versa; also the params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conf = fit_text(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# set up the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up a precomputed embeddings layer; glove based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "MAX_NUM_WORDS = 20000\n",
    "EMBEDDING_DIM = 100\n",
    "\n",
    "texts = X\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print(f'Found {len(word_index)} unique tokens.')\n",
    "\n",
    "embeddings_index = {}\n",
    "with open('glove.6B.100d.txt', 'r', encoding=\"utf8\") as fh:\n",
    "    for line in fh:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "\n",
    "print(f'Found {len(embeddings_index)} word vectors.')\n",
    "\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Params:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "HIDDEN_UNITS = 100\n",
    "MAX_DECODER_SEQ_LENGTH = 4\n",
    "\n",
    "input_word2idx = conf['input_word2idx']\n",
    "input_idx2word = conf['input_idx2word']\n",
    "target_word2idx = conf['target_word2idx'] \n",
    "target_idx2word = conf['target_idx2word']\n",
    "num_input_tokens = conf['num_input_tokens']\n",
    "num_target_tokens = conf['num_target_tokens']\n",
    "max_input_seq_length = conf['max_input_seq_length']\n",
    "max_target_seq_length = conf['max_target_seq_length']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs1 = Input(shape=( max_input_seq_length,))\n",
    "article1 = Embedding(len(word_index) + 1, EMBEDDING_DIM, weights=[embedding_matrix], input_length=max_input_seq_length,\n",
    "                    trainable=True)(inputs1) #premade weights, fine tune\n",
    "article2 = Dropout(0.3)(article1)\n",
    "\n",
    "# summary input model\n",
    "inputs2 = Input(shape=(min( num_target_tokens, MAX_DECODER_SEQ_LENGTH), ))\n",
    "summ1 = Embedding( num_target_tokens, EMBEDDING_DIM)(inputs2)\n",
    "summ2 = Dropout(0.3)(summ1)\n",
    "summ3 = LSTM(EMBEDDING_DIM)(summ2)\n",
    "summ4 = RepeatVector( max_input_seq_length)(summ3)\n",
    "\n",
    "# decoder model\n",
    "decoder1 = concatenate([article2, summ4])\n",
    "decoder2 = LSTM(EMBEDDING_DIM)(decoder1)\n",
    "outputs = Dense( num_target_tokens, activation='softmax')(decoder2)\n",
    "# tie it together [article, summary] [word]\n",
    "model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_batch(x_samples, y_samples, batch_size):\n",
    "    \"\"\"Generates the batches from the training data given the \n",
    "    batch size necessary; pads sequences if required\"\"\"\n",
    "    \n",
    "    encoder_input_data_batch = []\n",
    "    decoder_input_data_batch = []\n",
    "    decoder_target_data_batch = []\n",
    "    line_idx = 0\n",
    "    while True:\n",
    "        for recordIdx in range(0, len(x_samples)):\n",
    "            target_words = y_samples[recordIdx]\n",
    "            x = x_samples[recordIdx]\n",
    "            decoder_input_line = []\n",
    "\n",
    "            for idx in range(0, len(target_words)-1):\n",
    "                w2idx = 0  # default [UNK]\n",
    "                w = target_words[idx]\n",
    "                if w in  target_word2idx:\n",
    "                    w2idx =  target_word2idx[w]\n",
    "                decoder_input_line = decoder_input_line + [w2idx]\n",
    "                decoder_target_label = np.zeros( num_target_tokens)\n",
    "                w2idx_next = 0\n",
    "                if target_words[idx+1] in  target_word2idx:\n",
    "                    w2idx_next =  target_word2idx[target_words[idx+1]]\n",
    "                if w2idx_next != 0:\n",
    "                    decoder_target_label[w2idx_next] = 1\n",
    "\n",
    "                decoder_input_data_batch.append(decoder_input_line)\n",
    "                encoder_input_data_batch.append(x)\n",
    "                decoder_target_data_batch.append(decoder_target_label)\n",
    "\n",
    "                line_idx += 1\n",
    "                if line_idx >= batch_size:\n",
    "                    yield [pad_sequences(encoder_input_data_batch,  max_input_seq_length),\n",
    "                           pad_sequences(decoder_input_data_batch,\n",
    "                                         min( num_target_tokens, MAX_DECODER_SEQ_LENGTH))], np.array(decoder_target_data_batch)\n",
    "                    line_idx = 0\n",
    "                    encoder_input_data_batch = []\n",
    "                    decoder_input_data_batch = []\n",
    "                    decoder_target_data_batch = []   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fit(Xtrain, Ytrain, Xtest, Ytest, epochs=20, batch_size=256):\n",
    "    \"\"\"Trains a model and returns a history so that the training\n",
    "    statistics could be monitored as well\"\"\"    \n",
    "    \n",
    "    checkpoint = ModelCheckpoint('model4_checkpoint.h5')\n",
    "    \n",
    "    Ytrain =  split_target_text(Ytrain)\n",
    "    Ytest =  split_target_text(Ytest)\n",
    "\n",
    "    Xtrain =  transform_input_text(Xtrain)\n",
    "    Xtest =  transform_input_text(Xtest)\n",
    "\n",
    "    train_gen =  generate_batch(Xtrain, Ytrain, batch_size)\n",
    "    test_gen =  generate_batch(Xtest, Ytest, batch_size)\n",
    "\n",
    "    total_training_samples = sum([len(target_text)-1 for target_text in Ytrain])\n",
    "    total_testing_samples = sum([len(target_text)-1 for target_text in Ytest])\n",
    "    train_num_batches = total_training_samples // batch_size\n",
    "    test_num_batches = total_testing_samples // batch_size\n",
    "\n",
    "    history =  model.fit_generator(generator=train_gen, steps_per_epoch=train_num_batches,\n",
    "                                       epochs=epochs,\n",
    "                                       verbose=1, validation_data=test_gen, validation_steps=test_num_batches,\n",
    "                                       callbacks=[checkpoint])\n",
    "    \n",
    "    model.save('model4')\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(input_text):\n",
    "    \"\"\"Creates the summary from the input sequence;\n",
    "    samples from the decoder until either the end token is reached\n",
    "    or the maximum output sequence length is reached\"\"\"\n",
    "    \n",
    "    input_seq = []\n",
    "    input_wids = []\n",
    "    for word in input_text.lower().split(' '):\n",
    "        idx = 1  # default [UNK]\n",
    "        if word in  input_word2idx:\n",
    "            idx =  input_word2idx[word]\n",
    "        input_wids.append(idx)\n",
    "    input_seq.append(input_wids)\n",
    "    input_seq = pad_sequences(input_seq,  max_input_seq_length)\n",
    "    start_token =  target_word2idx['START']\n",
    "    wid_list = [start_token]\n",
    "    sum_input_seq = pad_sequences([wid_list], min( num_target_tokens, MAX_DECODER_SEQ_LENGTH))\n",
    "    terminated = False\n",
    "\n",
    "    target_text = ''\n",
    "\n",
    "    while not terminated:\n",
    "        output_tokens =  model.predict([input_seq, sum_input_seq])\n",
    "        sample_token_idx = np.argmax(output_tokens[0, :])\n",
    "        sample_word =  target_idx2word[sample_token_idx]\n",
    "        wid_list = wid_list + [sample_token_idx]\n",
    "\n",
    "        if sample_word != 'START' and sample_word != 'END':\n",
    "            target_text += ' ' + sample_word\n",
    "        if sample_word == 'END' or len(wid_list) >=  max_target_seq_length:\n",
    "            terminated = True\n",
    "        else:\n",
    "            sum_input_seq = pad_sequences([wid_list], min( num_target_tokens, MAX_DECODER_SEQ_LENGTH))\n",
    "    return target_text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = fit(Xtrain, Ytrain, Xtest, Ytest, epochs=20, batch_size=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "accuracy on the validation set reached in our case after 20 epochs was 0.2430"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model(r'model') #if exists; point to your path if different\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('titlesAbstracts_AT.pkl', 'rb') as fh:\n",
    "    titles = pickle.load(fh)\n",
    "\n",
    "with open('abstractsCorpus_ATsigns.pkl', 'rb') as fh:\n",
    "    text = pickle.load(fh)\n",
    "\n",
    "textConcat = list()\n",
    "\n",
    "from itertools import chain\n",
    "for each in text.values():\n",
    "    tmp = ' '.join(list(chain.from_iterable(each)))\n",
    "    textConcat.append(tmp)\n",
    "\n",
    "X = textConcat[:10000]\n",
    "Y = list(titles.values())[:10000]\n",
    "\n",
    "Xfull = textConcat\n",
    "Yfull = list(titles.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conf = fit_text(X, Y)\n",
    "\n",
    "HIDDEN_UNITS = 100\n",
    "\n",
    "MAX_DECODER_SEQ_LENGTH = 4\n",
    "\n",
    "input_word2idx = conf['input_word2idx']\n",
    "input_idx2word = conf['input_idx2word']\n",
    "target_word2idx = conf['target_word2idx'] \n",
    "target_idx2word = conf['target_idx2word']\n",
    "num_input_tokens = conf['num_input_tokens']\n",
    "num_target_tokens = conf['num_target_tokens']\n",
    "max_input_seq_length = conf['max_input_seq_length']\n",
    "max_target_seq_length = conf['max_target_seq_length']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get random article to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "c = randint(10000, len(Xfull))\n",
    "\n",
    "textPredict = Xfull[c]\n",
    "labelPredict = Yfull[c]\n",
    "\n",
    "summarize(textPredict)\n",
    "labelPredict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sumeval.metrics.rouge import RougeCalculator\n",
    "rouge = RougeCalculator(stopwords=True, lang=\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rouge1 = dict()\n",
    "\n",
    "for _ in range(50):\n",
    "    from random import randint\n",
    "    c = randint(10000, len(Xfull))\n",
    "\n",
    "    textPredict = Xfull[c]\n",
    "    labelPredict = Yfull[c]\n",
    "\n",
    "    generated = summarize(textPredict)\n",
    "    reference = labelPredict\n",
    "\n",
    "    score = rouge.rouge_n(\n",
    "                summary=generated,\n",
    "                references=reference,\n",
    "                n=1)\n",
    "    \n",
    "    rouge1[score] = (generated, reference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for s in sorted(rouge1.keys(), reverse=True):\n",
    "    \n",
    "    gen, orig = rouge1[s]\n",
    "    print(s)\n",
    "    print(f'Generated headline:{gen}')\n",
    "    print(f'Original headline:{orig}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rouge1 avg:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(list(rouge1.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rouge1 example on ideal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge.rouge_n(summary='I would like an apple.', references='I would like an apple.', n=1)\n",
    "rouge.rouge_n(summary='I would like to eat an apple.', references='I feel like having an apple.', n=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
